{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f101e208-c21a-40bc-aea7-74f0c2564e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_graph import gen_graph, validate_graph\n",
    "from typing import TypedDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a15d16-b645-4735-890a-c57b18702885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State\n",
    "class Tally(TypedDict):\n",
    "    topic: str\n",
    "    human_chooses_openai_count: int\n",
    "    human_chooses_anthropic_count: int\n",
    "    tie_count: int\n",
    "    openai_response: str\n",
    "    anthropic_response: str\n",
    "    openai_is_first: bool\n",
    "\n",
    "# Conditions\n",
    "def is_done(state):\n",
    "    return state['topic'].lower() == 'q'\n",
    "\n",
    "# Nodes\n",
    "def human_input(state):\n",
    "    topic = input(\"Choose a joke topic, or q to quit: \")\n",
    "    if not 'human_chooses_openai_count' in state:\n",
    "        state['human_chooses_openai_count'] = 0\n",
    "    if not 'human_chooses_anthropic_count' in state:\n",
    "        state['human_chooses_anthropic_count'] = 0\n",
    "    if not 'tie_count' in state:\n",
    "        state['tie_count'] = 0\n",
    "    tie_count = state['tie_count']\n",
    "    openai_count = state['human_chooses_openai_count']\n",
    "    anthropic_count = state['human_chooses_anthropic_count']\n",
    "    return { \"topic\": topic, 'human_chooses_openai_count': openai_count, 'human_chooses_anthropic_count': anthropic_count, 'tie_count': tie_count }\n",
    "\n",
    "def call_openai(state):\n",
    "    response = openai_llm.invoke(f\"tell me a joke about {state['topic']}.  Just tell the joke.  Don't introduce it, and don't explain it.  Only the joke.\")\n",
    "    return { 'openai_response': response.content }\n",
    "\n",
    "def call_anthropic(state):\n",
    "    response = anthropic_llm.invoke(f\"tell me a joke about {state['topic']}.  Just tell the joke.  Don't introduce it, and don't explain it.  Only the joke.\")\n",
    "    return { 'anthropic_response': response.content }\n",
    "\n",
    "def randomize_for_human(state):\n",
    "    return { 'openai_is_first': random.choice([True, False]) }\n",
    "\n",
    "def human_chooses(state):\n",
    "    print(f\"1. {state['openai_response'] if state['openai_is_first'] else state['anthropic_response']}\")\n",
    "    print(f\"2. {state['openai_response'] if not state['openai_is_first'] else state['anthropic_response']}\")\n",
    "    print(\"3. tied\")\n",
    "    choice = input(\"choose 1,2, or 3: \")\n",
    "    if choice == \"1\" and state['openai_is_first']:\n",
    "        return { 'human_chooses_openai_count': state['human_chooses_openai_count'] + 1 }\n",
    "    if choice == \"1\" and not state['openai_is_first']:\n",
    "        return { 'human_chooses_anthropic_count': state['human_chooses_anthropic_count'] + 1 }\n",
    "    if choice == \"2\" and state['openai_is_first']:\n",
    "        return { 'human_chooses_anthropic_count': state['human_chooses_anthropic_count'] + 1 }\n",
    "    if choice == \"2\" and not state['openai_is_first']:\n",
    "        return { 'human_chooses_openai_count': state['human_chooses_openai_count'] + 1 }\n",
    "    return { 'tie_count': state['tie_count'] + 1 }\n",
    "\n",
    "def show_tally(state):\n",
    "    print(f\"Open AI preferred:   {state['human_chooses_openai_count']}\")\n",
    "    print(f\"Anthropic preferred: {state['human_chooses_anthropic_count']}\")\n",
    "    print(f\"no preference: {state['tie_count']}\")\n",
    "    return state\n",
    "    \n",
    "graph_spec = \"\"\"\n",
    "\n",
    "START(Tally) => human_input\n",
    "\n",
    "human_input\n",
    "  is_done => show_tally\n",
    "  => call_openai, call_anthropic\n",
    "\n",
    "call_openai, call_anthropic => randomize_for_human\n",
    "\n",
    "randomize_for_human => human_chooses\n",
    "\n",
    "human_chooses => human_input\n",
    "\n",
    "show_tally => END\n",
    "\"\"\"\n",
    "graph_code = gen_graph(\"tally\", graph_spec)\n",
    "result = validate_graph(graph_spec)\n",
    "# result will have \"errors\" if there were errors, and \"solutions\" if there are solutions, and \"graph\" representing the graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a5d0161-b5e0-40aa-8f9f-a375f0c95dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'graph': {'START': {'state': 'Tally',\n",
       "   'edges': [{'condition': 'true_fn', 'destination': 'human_input'}]},\n",
       "  'human_input': {'state': 'Tally',\n",
       "   'edges': [{'condition': 'is_done', 'destination': 'show_tally'},\n",
       "    {'condition': 'true_fn', 'destination': 'call_openai, call_anthropic'}]},\n",
       "  'call_openai, call_anthropic': {'state': 'Tally',\n",
       "   'edges': [{'condition': 'true_fn', 'destination': 'randomize_for_human'}]},\n",
       "  'randomize_for_human': {'state': 'Tally',\n",
       "   'edges': [{'condition': 'true_fn', 'destination': 'human_chooses'}]},\n",
       "  'human_chooses': {'state': 'Tally',\n",
       "   'edges': [{'condition': 'true_fn', 'destination': 'human_input'}]},\n",
       "  'show_tally': {'state': 'Tally',\n",
       "   'edges': [{'condition': 'true_fn', 'destination': 'END'}]}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d3d49d-0f33-4f0c-ab53-893db6dadd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Type, List\n",
    "import inspect\n",
    "\n",
    "def check_typeddicts(names: List[str]) -> List[bool]:\n",
    "    \"\"\"\n",
    "    Check if TypedDict classes with the given names exist in the current namespace.\n",
    "    \n",
    "    Args:\n",
    "        names: List of class names to check\n",
    "        \n",
    "    Returns:\n",
    "        List[bool]: List of boolean values indicating existence of each TypedDict\n",
    "    \"\"\"\n",
    "    # Get all variables in the current namespace\n",
    "    current_namespace = globals()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name in names:\n",
    "        # Check if name exists and is a TypedDict\n",
    "        if name in current_namespace:\n",
    "            obj = current_namespace[name]\n",
    "            \n",
    "            # Check if it's a class\n",
    "            if not isinstance(obj, type):\n",
    "                results.append(False)\n",
    "                continue\n",
    "                \n",
    "            # Check if it's a TypedDict\n",
    "            try:\n",
    "                is_typeddict = (issubclass(obj, dict) and \n",
    "                               hasattr(obj, '__annotations__') and \n",
    "                               getattr(obj, '_is_protocol', False) is False)\n",
    "                results.append(is_typeddict)\n",
    "            except TypeError:\n",
    "                results.append(False)\n",
    "        else:\n",
    "            results.append(False)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc47299-cf05-454d-bba0-0958624940b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import inspect\n",
    "\n",
    "def is_function_available(func_name):\n",
    "    \"\"\"\n",
    "    Check if a function with the given name is available and is a function object.\n",
    "\n",
    "    Args:\n",
    "        func_name: The name of the function to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the function exists and is a function object, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        func = eval(func_name)\n",
    "        return inspect.isfunction(func)\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "def check_functions(names: List[str]) -> List[bool]:\n",
    "    \"\"\"\n",
    "    Check if functions with the given names exist in the current namespace.\n",
    "\n",
    "    Args:\n",
    "        names: List of function names to check.\n",
    "\n",
    "    Returns:\n",
    "        List[bool]: List indicating the existence of each function.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for name in names:\n",
    "        is_available = is_function_available(name)\n",
    "        results.append(is_available)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6287395c-ecbe-48ba-9822-38ab85a4e036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, False]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_typeddicts(['Tally', 'Cigarette', 'human_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "111f5407-3c32-41b2-a9b8-28625d54a19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, True]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_functions(['Tally', 'Cigarette', 'human_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7e7c39-eb58-4710-89ca-b3e9e16452db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'graph': {'START': {'state': 'Tally',\n",
       "   'edges': [{'condition': 'true_fn', 'destination': 'human_input'}]},\n",
       "  'human_input': {'state': 'Tally',\n",
       "   'edges': [{'condition': 'is_done', 'destination': 'show_tally'},\n",
       "    {'condition': 'true_fn', 'destination': 'call_openai, call_anthropic'}]},\n",
       "  'call_openai, call_anthropic': {'state': 'Tally',\n",
       "   'edges': [{'condition': 'true_fn', 'destination': 'randomize_for_human'}]},\n",
       "  'randomize_for_human': {'state': 'Tally',\n",
       "   'edges': [{'condition': 'true_fn', 'destination': 'human_chooses'}]},\n",
       "  'human_chooses': {'state': 'Tally',\n",
       "   'edges': [{'condition': 'true_fn', 'destination': 'human_input'}]},\n",
       "  'show_tally': {'state': 'Tally',\n",
       "   'edges': [{'condition': 'true_fn', 'destination': 'END'}]}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f0172b-2f1c-4d5e-b2e3-2a5c89c0dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_functions(graph_data):\n",
    "    \"\"\"\n",
    "    Extract node functions and condition functions from a graph dictionary,\n",
    "    excluding special cases ('START' node and 'true_fn' condition).\n",
    "    Handles multiple node names separated by commas.\n",
    "    \n",
    "    Args:\n",
    "        graph_data (dict): Dictionary containing graph structure\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (list of node functions, list of condition functions)\n",
    "    \"\"\"\n",
    "    node_functions = set()\n",
    "    condition_functions = set()\n",
    "    \n",
    "    # Add all keys from the graph dict as node functions, except 'START'\n",
    "    for key in graph_data['graph'].keys():\n",
    "        if key != 'START':\n",
    "            # Split key on commas and strip whitespace\n",
    "            node_functions.update(name.strip() for name in key.split(','))\n",
    "    \n",
    "    # Iterate through each node in the graph\n",
    "    for node in graph_data['graph'].values():\n",
    "        # Process each edge in the node\n",
    "        for edge in node['edges']:\n",
    "            # Add destination to node functions\n",
    "            # Split on commas and strip whitespace\n",
    "            destinations = edge['destination'].split(',')\n",
    "            node_functions.update(name.strip() for name in destinations)\n",
    "            \n",
    "            # Add condition to condition functions if not 'true_fn'\n",
    "            if edge['condition'] != 'true_fn':\n",
    "                condition_functions.add(edge['condition'])\n",
    "    \n",
    "    return (sorted(list(node_functions)), sorted(list(condition_functions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceef8f4b-3e46-4f5c-8ec6-8ad75a1335f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, conditions = extract_functions(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "691b4d5c-5e1d-4beb-ab12-acc062c1ddee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['END',\n",
       " 'call_anthropic',\n",
       " 'call_openai',\n",
       " 'human_chooses',\n",
       " 'human_input',\n",
       " 'randomize_for_human',\n",
       " 'show_tally']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43e091f5-91c5-4d94-992e-5c5330274b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, True, True, True, True, True]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_functions(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "854b278c-94d5-4623-9477-099eb6a20759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "anthropic_llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n",
    "openai_llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34c93ea0-4582-4afa-b154-6fc8f9706249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai_response': 'Why did the Cheetos go to school? They wanted to be a little \"cheddar\"!'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_openai({\"topic\": \"cheetos\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "215b15b6-75ab-45bd-ba4a-88d65109a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(graph_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25b6eb19-b3e5-4e90-97d2-a00a4e8f51c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# GENERATED code, creates compiled graph: tally\n",
      "from langgraph.graph import START, END, StateGraph\n",
      "\n",
      "tally = StateGraph(Tally)\n",
      "tally.add_node('human_input', human_input)\n",
      "tally.add_node('call_openai', call_openai)\n",
      "tally.add_node('call_anthropic', call_anthropic)\n",
      "tally.add_node('randomize_for_human', randomize_for_human)\n",
      "tally.add_node('human_chooses', human_chooses)\n",
      "tally.add_node('show_tally', show_tally)\n",
      "tally.add_edge(START, 'human_input')\n",
      "def after_human_input(state: Tally):\n",
      "    if is_done(state):\n",
      "        return 'show_tally'\n",
      "    return ['call_openai', 'call_anthropic']\n",
      "\n",
      "human_input_conditional_edges = ['call_openai', 'call_anthropic', 'show_tally']\n",
      "tally.add_conditional_edges('human_input', after_human_input, human_input_conditional_edges)\n",
      "\n",
      "tally.add_edge(['call_openai','call_anthropic'], 'randomize_for_human')\n",
      "tally.add_edge('randomize_for_human', 'human_chooses')\n",
      "tally.add_edge('human_chooses', 'human_input')\n",
      "tally.add_edge('show_tally', END)\n",
      "\n",
      "tally = tally.compile()\n"
     ]
    }
   ],
   "source": [
    "print(graph_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d199a721-7f67-4af7-b22c-270d9b59e8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose a joke topic, or q to quit:  earphones\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Why did the earphones break up? They just couldn't find common groundâ€”one was always plugged in, and the other was constantly tangled in its own thoughts.\n",
      "2. Why did the earphones get fired from their job? They kept getting tangled up in office drama.\n",
      "3. tied\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "choose 1,2, or 3:  3\n",
      "Choose a joke topic, or q to quit:  dogs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Why did the dog sit in the shade?  \n",
      "Because he didn't want to be a hot dog!\n",
      "2. What do you call a dog magician? A labracadabrador.\n",
      "3. tied\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "choose 1,2, or 3:  2\n",
      "Choose a joke topic, or q to quit:  toes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Why did the toe always get in trouble? Because it couldn't toe the line!\n",
      "2. What did the toe say when it stubbed itself? Oh snap!\n",
      "3. tied\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "choose 1,2, or 3:  2\n",
      "Choose a joke topic, or q to quit:  cringe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Why did the cringe cross the road? To avoid being seen on both sides!\n",
      "2. Why did the teenager delete all their old social media posts? Because their cringe compilation had become a feature-length film.\n",
      "3. tied\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "choose 1,2, or 3:  3\n",
      "Choose a joke topic, or q to quit:  computers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Why do programmers always mix up Halloween and Christmas? Because Oct 31 = Dec 25.\n",
      "2. Why was the computer cold? It left its Windows open!\n",
      "3. tied\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "choose 1,2, or 3:  1\n",
      "Choose a joke topic, or q to quit:  iphones\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Why did the iPhone go to therapy? It had too many emotional baggage apps.\n",
      "2. Why did the iPhone go to therapy? It couldn't find its home button.\n",
      "3. tied\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "choose 1,2, or 3:  1\n",
      "Choose a joke topic, or q to quit:  hats\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tally\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheese\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py312/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1600\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1599\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1602\u001b[0m     config,\n\u001b[1;32m   1603\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1604\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1605\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1606\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1607\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1609\u001b[0m ):\n\u001b[1;32m   1610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1611\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py312/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1348\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_of_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1340\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[1;32m   1341\u001b[0m         message\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1342\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mGRAPH_RECURSION_LIMIT,\n\u001b[1;32m   1347\u001b[0m     )\n\u001b[0;32m-> 1348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(loop\u001b[38;5;241m.\u001b[39moutput)\n",
      "\u001b[0;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "tally.invoke({'topic': 'cheese'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88eb22b-eaa2-4015-8636-706ff2f2ce5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
